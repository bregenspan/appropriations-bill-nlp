{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the most-mentioned countries in the 2021 US spending omnibus\n",
    "## (A sloppy attempt by someone who is a mostly frontend engineer who hasn't worked with Python in awhile and doesn't know enough about NLP)\n",
    "\n",
    "## 1. Read bill text\n",
    "\n",
    "To get the text file used here, I extracted the bill's plaintext from this [source PDF](https://docs.house.gov/billsthisweek/20201221/BILLS-116HR133SA-RCP-116-68.pdf) via xpdf's `pdftotext`. Unfortunately, this leaves the line numbering in place, and there are hyphenated word breaks that cross lines. I cleaned up a bit to unwrap words that got wrapped with a hyphen across multiple lines by removing all matches of this regex:\n",
    "\n",
    "```\n",
    "-$\\n+\\d+\\s+\n",
    "```\n",
    "\n",
    "The text version is still imperfect after that, with some multiword country names potentially split across multiple lines, possibly causing some country references to be broken, but I don't expect this to affect the count too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2483930\n"
     ]
    }
   ],
   "source": [
    "bill_text = open('public-law-116-94.txt', 'r', encoding=\"ISO-8859-1\").read()\n",
    "print(len(bill_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use `flashgeotext` to get (rough) country counts\n",
    "\n",
    "This bill is very big, almost 7 million characters. I tried working with [Spacy](https://spacy.io/) but had to break up the work into chunks so as not to hit memory limits, and ultimately didn't get a great result for various other reasons, mostly patience (Spacy's stock models aren't trained to extract *just* countries, instead extracting \"geopolitical entities\", and I got impatient with its runtime while trying to figure out how to appropriately filter just to countries).\n",
    "\n",
    "[flashgeotext](https://github.com/iwpnd/flashgeotext) is simpler and looks more like what I wanted: just produce a count of mentions of countries with no intermediate steps required, and a much faster search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashgeotext.geotext import GeoText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-22 23:31:17.438 | DEBUG    | flashgeotext.lookup:add:194 - cities added to pool\n",
      "2020-12-22 23:31:17.553 | DEBUG    | flashgeotext.lookup:add:194 - countries added to pool\n",
      "2020-12-22 23:31:17.554 | DEBUG    | flashgeotext.lookup:_add_demo_data:225 - demo data loaded for: ['cities', 'countries']\n"
     ]
    }
   ],
   "source": [
    "geotext = GeoText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = geotext.extract(bill_text, span_info=False).get('countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'United States': {'count': 1112}, 'Mexico': {'count': 11}, 'Puerto Rico': {'count': 52}, 'China': {'count': 62}, 'Russia': {'count': 33}, 'Palau': {'count': 1}, 'Iran': {'count': 15}, 'North Korea': {'count': 11}, 'South Korea': {'count': 1}, 'Japan': {'count': 2}, 'Cuba': {'count': 15}, 'Afghanistan': {'count': 23}, 'Taiwan': {'count': 2}, 'Canada': {'count': 8}, 'United Kingdom': {'count': 2}, 'Israel': {'count': 49}, 'Bahrain': {'count': 1}, 'Myanmar': {'count': 18}, 'Cambodia': {'count': 12}, 'Colombia': {'count': 15}, 'Egypt': {'count': 16}, 'Ethiopia': {'count': 1}, 'Iraq': {'count': 17}, 'Lebanon': {'count': 8}, 'Pakistan': {'count': 6}, 'Philippines': {'count': 3}, 'Sudan': {'count': 13}, 'Sri Lanka': {'count': 5}, 'Zimbabwe': {'count': 4}, 'Jordan': {'count': 6}, 'Tunisia': {'count': 4}, 'Ukraine': {'count': 12}, 'Palestine': {'count': 13}, 'Morocco': {'count': 2}, 'Saudi Arabia': {'count': 5}, 'Cameroon': {'count': 3}, 'Central African Republic': {'count': 2}, 'Democratic Republic of Congo': {'count': 3}, 'Chad': {'count': 2}, 'Malawi': {'count': 2}, 'Thailand': {'count': 1}, 'Vietnam': {'count': 5}, 'India': {'count': 1}, 'Nepal': {'count': 6}, 'Bangladesh': {'count': 3}, 'Belize': {'count': 2}, 'Costa Rica': {'count': 4}, 'Peru': {'count': 2}, 'Trinidad and Tobago': {'count': 1}, 'Georgia': {'count': 3}, 'Turkey': {'count': 3}, 'Abkhazia': {'count': 3}, 'Serbia': {'count': 1}, 'Iceland': {'count': 3}, 'Norway': {'count': 3}, 'Chile': {'count': 2}, 'Bolivia': {'count': 1}, 'France': {'count': 1}, 'Germany': {'count': 1}, 'Italy': {'count': 1}, 'Spain': {'count': 1}, 'Portugal': {'count': 1}, 'Sweden': {'count': 1}, 'Netherlands': {'count': 1}, 'Argentina': {'count': 1}, 'Brazil': {'count': 1}, 'Greece': {'count': 29}, 'Bulgaria': {'count': 1}, 'Guinea-Bissau': {'count': 2}}\n"
     ]
    }
   ],
   "source": [
    "print(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spot-check\n",
    "\n",
    "Spot-checking the above just by searching inside the original PDFs, the results look decent. But there are some issues that would require a fancier approach to fix: the \"Sudan\" count includes South Sudan mentions (outdated country list?), and \"Mexico\" includes New Mexico, which bloats the count for mentions of Mexico-the-country to ~25% higher than it should be. The model isn't aware of US states so it's failing to eliminate New Mexico as a longer string match. So we need to go at least a bit fancier to get a good result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Try wiring up `pycountry`'s country database + `pyahocorasick`\n",
    "\n",
    "flashgeotext uses an [Ahoâ€“Corasick algorithm](https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm) implementation under the hood for its string searching. Let's drop down one level of abstraction and re-implement to handle the country lookup we need by building an \"automaton\" more directly, using [`pyahocorasick`](https://pypi.org/project/pyahocorasick/). Reading around, it looks like it might have better handling for overlapping matches than the implementation used in flashgeotext.\n",
    "\n",
    "We basically do this by dumping [`pycountry`](https://pypi.org/project/pycountry/)'s database of countries and US subdivisions/states into ahocorasick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ahocorasick\n",
    "import pycountry\n",
    "\n",
    "A = ahocorasick.Automaton()\n",
    "\n",
    "# Add countries to the automaton\n",
    "for country in pycountry.countries:    \n",
    "    # `country.name` is the [ISO English \"short country name\"](https://unstats.un.org/unsd/tradekb/knowledgebase/country-code).\n",
    "    # Unfortunately, it's not always all that short. We need to special-case to handle some countries with long \"short\" names\n",
    "    # that are commonly abbreviated further:\n",
    "    shorter_names = {\n",
    "        'Russian Federation': 'Russia',\n",
    "        'Korea, Republic of': 'South Korea',\n",
    "        \"Korea, Democratic People's Republic of\": 'North Korea'\n",
    "    }\n",
    "\n",
    "    shorter_name = None\n",
    "    \n",
    "    if country.name in shorter_names:\n",
    "        shorter_name = shorter_names[country.name]\n",
    "    elif \",\" in country.name:\n",
    "        # Catch-all for other long short names\n",
    "        # e.g. \"Iran, Islamic Republic of\" => \"Iran\"; \"Bolivia, Plurinational State of\" => \"Bolivia\"\n",
    "        # FIXME: Should manually list these out instead in shorter_names because there are edge cases here like\n",
    "        # \"Saint Helena, Ascension and Tristan da Cunha\"\n",
    "        shorter_name = country.name.split(\",\")[0]\n",
    "    \n",
    "    # There are some political implications to choosing ISO English short name representation\n",
    "    # or more common vernacular. For example, Taiwan is \"Taiwan, Province of China\" per its\n",
    "    # ISO short name. Here let's opt for the most commonly-used naming, which also\n",
    "    # lines up with most of the naming in the bill text in this case.\n",
    "    entity = ('COUNTRY', shorter_name or country.name,)\n",
    "    \n",
    "    A.add_word(country.name, entity)\n",
    "    if shorter_name is not None:\n",
    "        A.add_word(shorter_name, entity)    \n",
    "    if hasattr(country, 'official_name'):\n",
    "        A.add_word(country.official_name, entity)\n",
    "\n",
    "# Add US states and territories\n",
    "for subdivision in pycountry.subdivisions.get(country_code='US'):\n",
    "    if subdivision.type == 'State':\n",
    "        A.add_word(subdivision.name, ('US_STATE', subdivision.name,))\n",
    "    else:\n",
    "        A.add_word(subdivision.name, ('US_TERRITORY', subdivision.name,))\n",
    "        \n",
    "A.make_automaton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through matches the automaton found; get counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States, 1062\n",
      "Venezuela, 90\n",
      "China, 56\n",
      "Israel, 49\n",
      "Cyprus, 47\n",
      "Russia, 33\n",
      "Greece, 29\n",
      "Afghanistan, 23\n",
      "Iraq, 17\n",
      "Egypt, 16\n",
      "Iran, 15\n",
      "Cuba, 15\n",
      "Colombia, 15\n",
      "Palestine, 13\n",
      "Cambodia, 12\n",
      "Mexico, 11\n",
      "North Korea, 11\n",
      "Ukraine, 11\n",
      "Hong Kong, 10\n",
      "Canada, 8\n",
      "Haiti, 8\n",
      "Lebanon, 8\n",
      "Sudan, 8\n",
      "Somalia, 6\n",
      "Honduras, 6\n",
      "Pakistan, 6\n",
      "Jordan, 6\n",
      "Nepal, 6\n",
      "Guatemala, 5\n",
      "South Sudan, 5\n",
      "Sri Lanka, 5\n",
      "Yemen, 5\n",
      "Saudi Arabia, 5\n",
      "Libya, 4\n",
      "Nicaragua, 4\n",
      "Zimbabwe, 4\n",
      "Tunisia, 4\n",
      "Costa Rica, 4\n",
      "Micronesia, 3\n",
      "El Salvador, 3\n",
      "Philippines, 3\n",
      "Cameroon, 3\n",
      "Congo, 3\n",
      "Bangladesh, 3\n",
      "Panama, 3\n",
      "Turkey, 3\n",
      "Iceland, 3\n",
      "Norway, 3\n",
      "Marshall Islands, 2\n",
      "Samoa, 2\n",
      "Northern Mariana Islands, 2\n",
      "Japan, 2\n",
      "Taiwan, 2\n",
      "Morocco, 2\n",
      "Central African Republic, 2\n",
      "Chad, 2\n",
      "Malawi, 2\n",
      "Belize, 2\n",
      "Peru, 2\n",
      "Ecuador, 2\n",
      "Chile, 2\n",
      "Guinea, 2\n",
      "Palau, 1\n",
      "Bahrain, 1\n",
      "Ethiopia, 1\n",
      "Uzbekistan, 1\n",
      "Western Sahara, 1\n",
      "Niger, 1\n",
      "Nigeria, 1\n",
      "Thailand, 1\n",
      "India, 1\n",
      "Trinidad and Tobago, 1\n",
      "Bolivia, 1\n",
      "France, 1\n",
      "Germany, 1\n",
      "Italy, 1\n",
      "Spain, 1\n",
      "Portugal, 1\n",
      "Sweden, 1\n",
      "Netherlands, 1\n",
      "United Kingdom, 1\n",
      "Uruguay, 1\n",
      "Argentina, 1\n",
      "Brazil, 1\n",
      "Guyana, 1\n",
      "Paraguay, 1\n",
      "Saint Lucia, 1\n",
      "Bulgaria, 1\n",
      "Uganda, 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "last_idx = None\n",
    "entities = []\n",
    "\n",
    "word_char = re.compile('\\w')\n",
    "\n",
    "for (idx, entity,) in A.iter(bill_text):\n",
    "    # Look ahead to next character. If it's a word character, we don't want this\n",
    "    # entity - it's a prefix match for something else (e.g. \"India\" in \"Indian\")\n",
    "    next_char = bill_text[idx + 1 : idx + 2]\n",
    "    if (word_char.match(next_char)):\n",
    "        continue\n",
    "\n",
    "    if idx == last_idx:\n",
    "        # Overlapping match found. Filter to the longest match; only replace previous match\n",
    "        # if new entity name is longer.\n",
    "        # (e.g. when \"New Mexico\" match is followed by \"Mexico\" - \"New Mexico\" wins)\n",
    "        if len(entity[1]) > len(entities[-1][1]):\n",
    "            entities[-1] = entity\n",
    "    else:\n",
    "        entities.append(entity)\n",
    "    last_idx = idx\n",
    "\n",
    "countries = [entity for entity in entities if entity[0] == 'COUNTRY']\n",
    "country_counts = {}\n",
    "for (_, country) in countries:\n",
    "    if country not in country_counts:\n",
    "        country_counts[country] = 1\n",
    "    else:\n",
    "        country_counts[country] += 1\n",
    "        \n",
    "country_counts_sorted = sorted(country_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print('\\n'.join([k[0] + ', ' + str(k[1]) for k in country_counts_sorted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This is where I stopped due to time constraints. It seems to be much more accurate than the attempt earlier in this notebook. Spot-checking it shows we seem to be matching genuine references to countries, not nationalities, and we're not confusing \"New Mexico\" with \"Mexico\". But:\n",
    "\n",
    " - \"Gulf of Mexico\" counts as a \"Mexico\" reference. To handle that, we need some way of disambiguating non-country, non-US-state geopolitical entities like \"Gulf of Mexico\". That would involve either semantic awareness of \"Gulf of Mexico\" as a concept, or syntactic awareness (Gulf of Mexico == proper noun that should trump the nested Mexico proper noun).\n",
    " - ~Much worse, \"North Korea\" is nowhere to be seen - I sloppily stripped the \", Republic of\" and \", Democratic People's Republic of\" suffixes from the ISO short name! This carelessly reunifies Korea.~\n",
    " \n",
    "For my purposes (practice exercise), I think I'll be happy with this after adding a new list of short country names that patches the ISO short names. This should fix the last issue, but not the first one. To get more accuracy, a fancier model with more semantic awareness is likely needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
